Email scraper by WhoamiGroot,

You need to install,

pip install lxml

email_scraper.py = only scrape from domain only (Legal scraping).

email_scraper_large.py = scrape the whole web, more illegal way.

✅ Strengths

    User-Agent Header: Helps avoid bot detection.
    Restricts Crawling to Target Domain: Prevents unnecessary external site scanning.
    Efficient Queue System (deque): Optimized for crawling.
    Handles Request Failures Gracefully: Uses try-except blocks to manage errors.
    Validates Content Type: Ensures non-HTML pages are skipped.
    Saves Extracted Emails to a File: Makes it easy to reference later.

Key Improvements:

 -   User-Agent Spoofing: Prevents easy detection by anti-bot measures.
 -   Better URL Handling: Uses urllib.parse.urljoin() to fix relative URLs.
 -   Error Handling: Uses requests.raise_for_status() to handle bad responses.
 -   More Efficient Checking: Prevents duplicate URL scraping.
 -   Timeout for Requests: Prevents hanging on slow websites.

✅ Improves URL handling (ensures absolute URLs, avoids duplicates)
✅ Adds headers (mimics a real browser to reduce blocking)
✅ Handles exceptions more gracefully


How It Works:

    User Input: Takes a target URL from the user.
    Set max pages instead of Dynamically. (You can still set max pages).
    URL Management: Uses a queue (deque) to manage URLs to be scraped.
    Set for Unique URLs: Keeps track of visited (scraped_urls) and found email addresses (emails).
    Loop Through URLs:
        Fetches the webpage using requests.
        Extracts email addresses using regex.
        Parses the page using BeautifulSoup to find new links.
        Adds new URLs to the queue.
    Stops at 100 URLs (modifiable).
    Handles Errors: Ignores missing schemas, connection issues, and allows for manual interruption (Ctrl+C).
    Outputs Found Emails.

Potential Improvements:

    Handle Robots.txt: Respect robots.txt to avoid violating site policies.
    Use Session for Efficiency: requests.Session() reduces redundant connections.
    Improve URL Handling: Some URLs may not be formatted correctly.
    Limit Scope: Avoid crawling external domains unless specified.
    Concurrency: Use asyncio or multiprocessing for faster scraping.
    User-Agent Spoofing: Some sites block bots; using headers can help.
    
    Final Thoughts

With these enhancements, your script will be:

    More efficient (avoiding duplicate URLs).
    More accurate (handling different email formats).
    More adaptable (user-defined settings).
    
    
Credit me: WhoamiGroot.
Respect the rules and use it for legal Ethical Hacking only or Bug Hunting.
I am not responsible for any misbehaviour of this tool.
You know the rules and so do i..
Happy hacking.
